---
layout: page
permalink: /theory/
---

## Theorie - Denken wie ein Computer

### Bits und Bytes 

Mit Informatik verbindet man Hacker, Tüftler und eine ewige lange Sequenz von Nullen und Einsen. 
Doch bedeuten die ganzen Nullen und Einsen in der Informatik?

Es geht um die Frage, wie ein Computer Information versteht und wahrnimmt.

Wir stellen Zahlen im <b>Dezimalsystem</b> dar:

- 321 = 3 * 100 + 2 * 10 + 1 * 1

D.h. wie haben 10 Möglichkeiten pro Zifferstelle und jede Zifferstelle wird mit 10 hoch etwas multipliziert.
Super für uns weil wir Zehn Finger haben. Für uns ist dieses System das natürlichste der Welt.

Doch ein Computer hat keine Zehn Finger - er nimmt nur eine einzige Sache war. Eingang und Ausgang. Strom an und Strom aus.
0 und 1.

Ein Zahlensystem mit nur zwei Möglichkeiten nennt man <b>binäres Zahlensystem</b>. Es ist das perfekte System für den Computer.

- 0 (dez) = 000 = 0 * 4 + 0 * 2 + 0 * 1
- 1 (dez) = 001 = 0 * 4 + 0 * 2 + 1 * 1
- 2 (dez) = 010 = 0 * 4 + 1 * 2 + 0 * 1
- 3 (dez) = 011 = 0 * 4 + 1 * 2 + 1 * 1
- 4 (dez) = 100 = 1 * 4 + 0 * 2 + 0 * 1

